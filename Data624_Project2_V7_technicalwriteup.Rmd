---
title: "DATA624_Project2"
output:
  prettydoc::html_pretty: null
  theme: architec
  toc: yes
  fig_caption: yes
  toc_collapsed: no
  toc_depth: 5
  highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache=TRUE)
#install.packages(c("corrplot", "ggcorrplot"))
#install.packages(c("VIM", "rpart", "randomForest", "party", "gbm", "Cubist", "rpart.plot"))
#install.packages("PerformanceAnalytics")
library(VIM)
library(Hmisc)
#Corr Plot
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(PerformanceAnalytics)
library(magrittr)
library(reshape2)
library(ggplot2)
#Data Imputation
library(mice)
#Train model
library(caret)
#Timing
library(tictoc)
#Tree Based Plots
library(rpart)
library(randomForest)
library(party)
library(gbm)
#library(pryr)
library(Cubist)
library(Hmisc)
library(mice)
library(magrittr)
#library(partykit)
library(rpart.plot)
#Parallel Processing
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#stopCluster(cl)
set.seed(1978)
```

```{r}
hist.data.frame <- function(x, ..., colors=rainbow(ncol(x))) {
    col<-1
    hist<-function(...) {
        graphics::hist(..., col=colors[col])
        col <<- col+1
    }
    f <- Hmisc:::hist.data.frame
    environment(f) <- environment()
    f(x,...)
}
```

## Project 2: PH Prediction

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.


First we load our data. The data was provided in an excel document but for reproducibility, we've uploaded it to github so anyone can use the link. The column, “Brand.Code” is a categorical variable and so we change the data type to a factor. 
```{r data}
training_loc <- "https://raw.githubusercontent.com/JMawyin/DATA_624/master/Student_Data_train.csv"
training_df <- read.csv(file = training_loc)
training_df$Brand.Code <- as.factor(training_df$Brand.Code)
test_loc <- "https://raw.githubusercontent.com/JMawyin/DATA_624/master/Student_Data_test.csv"
test_df <- read.csv(file = test_loc)
head(training_df)
```

In the next sections we do some exploratory data analysis. We get a list of the column names and explore the completeness of the data. One pattern we look for is to see if some rows or columns are more incomplete than others. These incomplete fields or records could be excluded or used as a dummy variable in the future.  
```{r}
cat("Dimensions of Training DF:\n",dim(training_df))
cat("\n\nName of columns in Dataframe:\n")
colnames(training_df)
```


```{r}
table(training_df$Brand.Code)
hist(training_df$PH)
```

```{r}
dim(training_df)
```

```{r}
colSums(is.na(training_df))
```

```{r}
plot <- aggr(training_df, col = c("green", "red"), 
             numbers = TRUE, 
             sortVars = TRUE)
```


```{r}
#Moving the PH column to the front of the dataframe
training_df <- training_df[,c(which(colnames(training_df)=="PH"),which(colnames(training_df)!="PH"))]
colSums(is.na(training_df))
```

Now we look at the distribution of each column. For modeling purposes, each column would idealy have a normal distribution so we are looking at which columns might be candidates for transformations.  

```{r}
hist(training_df[,1:9])
```

```{r}
hist(training_df[,10:18])
```

```{r}
hist(training_df[,19:27])
```

```{r}
hist(training_df[,28:33])
```

```{r}
boxplot(training_df$Filler.Speed)
```

We notice many columns are normally distributed, some have bimodal distributions, some are skewed with a long tail of outliers, and some only have values in intervals of 2’s or 10’s. In our model pre-processing, we may choose different transformations to normalize and standardize our data. One pattern that stands out is that many columns have a high number of zero variables. We can assume that there is some significance to these high number of zeros and there effect may be linear in nature. For these data we will create dummy variables based on a specified cutoff value.


```{r}
training_df_binary <- training_df
training_df_binary$Air.Pressurer_bin <- ifelse(training_df_binary$Air.Pressurer > 145, 1, 0)
training_df_binary$Balling.Lvl_bin <- ifelse(training_df_binary$Balling.Lvl > 2, 1, 0)
training_df_binary$Balling_bin <- ifelse(training_df_binary$Balling > 2.5, 1, 0)
training_df_binary$Density_bin <- ifelse(training_df_binary$Density > 1.25, 1, 0)
training_df_binary$Hyd.Pressure1_bin <- ifelse(training_df_binary$Hyd.Pressure1 == 0, 0, 1)
training_df_binary$Hyd.Pressure2_bin <- ifelse(training_df_binary$Hyd.Pressure2 == 0, 0, 1)
training_df_binary$Hyd.Pressure3_bin <- ifelse(training_df_binary$Hyd.Pressure3 == 0, 0, 1)
training_df_binary$Mnf.Flow_bin <- ifelse(training_df_binary$Mnf.Flow > 50, 0, 1)
dim(training_df_binary)
#hist(training_df_binary)
#hist(training_df_binary$Hyd.Pressure3)
```

Here we observe the correlation between variables. Variables that are highly correlated offer limited additional insights for our model. Non-linear models typically handle these highly correlated values better than linear models. When looking for the most influential variables on our outcome variable, we will have to keep these in mind as well.

```{r}
corr <- model.matrix(~0+., data=training_df_binary) %>% 
  cor(use="pairwise.complete.obs")
ggplot(melt(corr), aes(Var1, Var2, fill=value)) +
  geom_tile(height=0.8, width=0.8) +
  scale_fill_gradient2(low="blue", mid="white", high="red") +
  theme_minimal() +
  coord_equal() +
  labs(x="",y="",fill="Corr") +
  theme(axis.text.x=element_text(size=8, angle=90, vjust=1, hjust=1, 
                                 margin=ggplot2::margin(-3,0,0,0)),
        axis.text.y=element_text(size=8, margin=ggplot2::margin(0,-3,0,0)),
        panel.grid.major=element_blank()) 
```

As noted earlier, there are many missing variables from our model. Since many of the models we are planning on using will only use complete cases, we need to impute the missing data points. Occasionally missing data, when manually entered, can be assumed to be zero but we do not believe this to be the case. Some methods for handling missing data are using the mean, median, or mode. We will use a method called multivariate imputation by chained equations or MICE. MICE is a great imputation method because it preserves the relations within the data and the uncertainty about those relations. We use the argument m=5 to indicate that we will do 5 imputations, each with the same dataset but different imputed values, that will then be analyzed then pooled to ensure relations and uncertainty is maintained. The method pmm is short for predictive mean matching. These imputations are restricted to observed values so this method works for our categorical variable as well. This will preserve non-linear relationships and it is computationally faster than other MICE methods. 

```{r}
#Imputing using the pmm mehtod, creating 5 dataframes, do not print process
imp_training <- mice(training_df_binary, m=5, method = 'pmm',print =  FALSE)
 
# checking the summary
summary(imp_training)
# Get complete data ( 3rd out of 5)
imp_training_df <- complete(imp_training,3)
colSums(is.na(imp_training_df))
head(imp_training_df)
#A BRIEF INTRODUCTION TO MICE R PACKAGE
#https://datasciencebeginners.com/2018/11/11/a-brief-introduction-to-mice-r-package/
```



Now we're going to remove the highly correlated predictor variables. First we turn our dataframe into a matrix object then determine the correlation between columns. We use the findCorrelation() function to make a list of columns to remove. The absolute values of pair-wise correlations are considered. If two variables have a high correlation, the function looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation. We're using the cutoff coefficient of .75.  This reduces the data from 41 columns to 26 columns.

Next we pre-process the data using center and scale. Center subtracts the mean of the predictor's data from the predictor values. This makes it easier to interpret the intercept. Scale divides by the predictor data by the standard deviation which will standardize the units of the regression coefficients. This pre-processing won't affect our estimates and our p-values will remain the same. 

```{r, eval=FALSE}
# #Removing Highly Correlated Predictors
descrCor <-  model.matrix(~0+., data=imp_training_df) %>%
  cor(use="pairwise.complete.obs")
summary(descrCor[upper.tri(descrCor)])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
imp_No_High_Corr_training_df <- imp_training_df[,-highlyCorDescr]
descrCor2 <- model.matrix(~0+., data=imp_No_High_Corr_training_df) %>%
  cor(use="pairwise.complete.obs")
summary(descrCor[upper.tri(descrCor)])
dim(imp_training_df)
dim(imp_No_High_Corr_training_df)
head(imp_No_High_Corr_training_df)
#Pre Prcessing Predictors
pp_no_nzv <- preProcess(imp_No_High_Corr_training_df[, 2:26],
                        method = c("center", "scale")) #no  "nzv" "BoxCox"  "YeoJohnson"
pp_no_nzv
Pre_processed_imp_No_High_Corr_training_df <- predict(pp_no_nzv, newdata = imp_No_High_Corr_training_df[, 1:26])
```

################ Trouble, changed 27 to 26 columns


Next we split our pre-processed dataset into a training set and a test set to evaluate our model's effectiveness. The training set is data 

```{r}
#training_df
#imp_training_df
#imp_No_High_Corr_training_df
#Pre_processed_imp_No_High_Corr_training_df
df <- imp_training_df
#df <- ppc_df
#Creating Test and Training Set
trainIndex <- createDataPartition(df$PH, p = .8, 
                                  list = FALSE, 
                                  times = 1)
df_Train <- df[ trainIndex,]
df_Test  <- df[-trainIndex,]
```



=================
```{r}
# model1 <- lm(PH ~., data = df_Train)
# predictions <- model1 %>% predict(df_Test)
# data.frame(
#   RMSE = RMSE(predictions, df_Test$PH),
#   R2 = R2(predictions, df_Test$PH)
# )
# car::vif(model1)
```
```{r}
# model2 <- lm(PH ~. -Carb.Pressure, data = df_Train)
# predictions2 <- model2 %>% predict(df_Test)
# data.frame(
#   RMSE = RMSE(predictions2, df_Test$PH),
#   R2 = R2(predictions2, df_Test$PH)
# )
# car::vif(model2)
```


=================

```{r}
tic()
######## Train Models
ctrl <- trainControl(method = "cv", number = 10)
#Linear Model
lm_model <- train(
  PH~., data = df_Train, method = "lm")
#Partial Least Squares
pls_model <- train(PH~., data = df_Train,
                  method = "pls",
                  ## The default tuning grid evaluates
                  ## components 1... tuneLength
                  tuneLength = 20,
                  trControl = ctrl,
                  preProc = c("center", "scale"))
#Ridge Regression
# Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(100)
ridge_model <- train(PH~., data = df_Train,
                      method = "ridge",
                      ## Fir the model over many penalty values
                      tuneGrid = ridgeGrid,
                      trControl = ctrl,
                      ## put the predictors on the same scale
                      preProc = c("center", "scale"))
#Robust Linear Regression
ctrl <- trainControl(method = "cv", number = 10)
rlmPCA_model <- train(PH~., data = df_Train,
                method = "rlm",
                preProcess = "pca",
                trControl = ctrl)
########  Make predictions
lm_predictions <- lm_model %>% predict(df_Test)
# Model performance metrics
lm_accuracy <- data.frame( 
  Model = "Linear Regression",
  RMSE = caret::RMSE(lm_predictions, df_Test$PH),
  Rsquare = caret::R2(lm_predictions, df_Test$PH)
)
#lm_accuracy
# Make predictions
pls_predictions <- pls_model %>% predict(df_Test)
# Model performance metrics
pls_accuracy <- data.frame( 
  Model = "Partial Least Squares",
  RMSE = caret::RMSE(pls_predictions, df_Test$PH),
  Rsquare = caret::R2(pls_predictions, df_Test$PH)
)
ridge_predictions <- ridge_model %>% predict(df_Test)
# Model performance metrics
ridge_accuracy <- data.frame( 
  Model = "Ridge-regression",
  RMSE = caret::RMSE(ridge_predictions, df_Test$PH),
  Rsquare = caret::R2(ridge_predictions, df_Test$PH)
)
rlmPCA_model_predictions <- rlmPCA_model %>% predict(df_Test)
cat("All four linear regression models took ") 
toc()
cat("to train.\n")
# Model performance metrics
rlmPCA_Accuracy <- data.frame(
  Model = "Robust Linear Model",
  RMSE = caret::RMSE(rlmPCA_model_predictions,df_Test$PH),
  Rsquare = caret::R2(rlmPCA_model_predictions,df_Test$PH))
rbind(lm_accuracy, rlmPCA_Accuracy, pls_accuracy, ridge_accuracy)
```
```{r}
dim(training_df)
dim(imp_training_df)
```


#Inputed Predictors and Binary Predictors
Linear Regression	0.1328836	0.4088347		
Robust Linear Model	0.1369765	0.3703304		
Partial Least Squares	0.1332510	0.4052933		
Ridge-regression	0.1330168	0.4070971	

Linear Regression	0.1294900	0.4370334		
Robust Linear Model	0.1325839	0.4110927		
Partial Least Squares	0.1296068	0.4370189		
Ridge-regression	0.1294900	0.4370334

Linear Regression	0.1294900	0.4370334		
Robust Linear Model	0.1325839	0.4110927		
Partial Least Squares	0.1296068	0.4370189		
Ridge-regression	0.1294900	0.4370334	

#Imputed Predictors
Linear Regression	0.1247379	0.4541905		
Robust Linear Model	0.1304918	0.4027846		
Partial Least Squares	0.1251459	0.4506813		
Ridge-regression	0.1249724	0.4521616		

Linear Regression	0.1191384	0.4442342		
Robust Linear Model	0.1267581	0.3699280		
Partial Least Squares	0.1195921	0.4402759		
Ridge-regression	0.1192468	0.4433601	

Linear Regression	0.1245448	0.4612370		
Robust Linear Model	0.1319684	0.3944299		
Partial Least Squares	0.1250159	0.4567820		
Ridge-regression	0.1248693	0.4585202	

Linear Regression	0.1398716	0.3504730		
Robust Linear Model	0.1436261	0.3131386		
Partial Least Squares	0.1397042	0.3519163		
Ridge-regression	0.1399046	0.3496677	

#imp_training_df
Linear Regression	0.1398716	0.3504730		
Robust Linear Model	0.1436261	0.3131386		
Partial Least Squares	0.1397042	0.3519163		
Ridge-regression	0.1399046	0.3496677

*imp_No_High_Corr_training_df
Linear Regression	0.1471077	0.2807598		
Robust Linear Model	0.1481916	0.2684600		
Partial Least Squares	0.1472959	0.2788179		
Ridge-regression	0.1473724	0.2779339

*Pre_processed_imp_No_High_Corr_training_df
Linear Regression	0.1481782	0.2720243		
Robust Linear Model	0.1474636	0.2758037		
Partial Least Squares	0.1478913	0.2743631		
Ridge-regression	0.1479631	0.2733337



Linear Regression	0.1481782	0.2720243		
Robust Linear Model	0.1474636	0.2758037		
Partial Least Squares	0.1478913	0.2743631		
Ridge-regression	0.1479631	0.2733337	

Linear Regression	0.1314113	0.3591871		
Robust Linear Model	0.1360182	0.3135105		
Partial Least Squares	0.1313908	0.3597019		
Ridge-regression	0.1312644	0.3599552	

Linear Regression	0.1357174	0.3887110		
Robust Linear Model	0.1398007	0.3522298		
Partial Least Squares	0.1354026	0.3915294		
Ridge-regression	0.1356460	0.3893101

```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(lm_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(pls_predictions, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
plot(ridge_predictions, df_Test$PH, ylab="Observed", col = "purple")
abline(0, 1, lwd=2)
plot(rlmPCA_model_predictions, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted", side = 3, line = -2, outer = TRUE)
```

```{r}
plot(varImp(lm_model), main="Linear Regression Predictor Importance", top = 5, xlim = c(0,100))
plot(varImp(ridge_model), main="Ridge Regression Predictor Importance", top = 5, xlim = c(0,100))
```



```{r}
tic()
knnModel <- train(PH~., data = df_Train,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)
#knnModel
tc <- trainControl(method = "cv",
                           number = 5,
                           classProbs = T)
svmModel <- train(PH~., data = df_Train,
                    method = "svmRadial",
                    preProcess = c("BoxCox","center", "scale"),
                    trControl = tc,
                    tuneLength = 9)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(PH~., data = df_Train,
                   method = "earth",
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = "cv"))
cat("All 3 non-linear regression models took ") 
toc()
cat("to train.\n")
```

```{r}
knnModel_predictions <- knnModel %>% predict(df_Test)
# Model performance metrics
knn_Accuracy <- data.frame(
  Model = "k-Nearest Neighbors",
  RMSE = caret::RMSE(knnModel_predictions,df_Test$PH),
  Rsquare = caret::R2(knnModel_predictions,df_Test$PH))
predictions_svm <- svmModel %>% predict(df_Test)
# Model SVM performance metrics
SMV_Acc <- data.frame(
  Model = "Support Vector Machine",
  RMSE = caret::RMSE(predictions_svm, df_Test$PH),
  Rsquare = caret::R2(predictions_svm, df_Test$PH)
)
#summary(marsTuned)
# Make MARS predictions
predictions_mars_tuned <- marsTuned %>% predict(df_Test)
# Model MARS performance metrics
MARS_Acc_tuned <- data.frame(
  Model = "MARS Tuned",
  RMSE = caret::RMSE(predictions_mars_tuned, df_Test$PH),
  Rsquare = caret::R2(predictions_mars_tuned, df_Test$PH)
)
names(MARS_Acc_tuned)[names(MARS_Acc_tuned) == 'y'] <- "Rsquare"
rbind(knn_Accuracy,SMV_Acc,MARS_Acc_tuned)
```

#Inputed Predictors and Binary Predictors

#Inputed Predictors
k-Nearest Neighbors	0.1082467	0.5914802		
Support Vector Machine	0.1066275	0.6045377		
MARS Tuned	0.1147101	0.5398671	

k-Nearest Neighbors	0.1266089	0.4806607		
Support Vector Machine	0.1203518	0.5272374		
MARS Tuned	0.1300447	0.4443375	

```{r}
ncol(df_Train)
5 * (ncol(df_Train) + 1) + 5 + 1
```

```{r}
NNModel_1 <- avNNet(PH~., data = df_Train,
                   size = 5, 
                   decay = 0.01,
                   linout = TRUE, 
                   trace = FALSE,
                   maxit = 500)
predictions_NNModel_1 <- NNModel_1 %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
predictions_NNModel_1_Acc
```


```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(knnModel_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(predictions_svm, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(predictions_mars_tuned, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted  - Non - Linar Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```


```{r, eval = FALSE}
NNModel_1 <- avNNet(PH~., data = df_Train,
                  #trainingData$x, trainingData$y,
                  size = 5,
                  decay = 0.01,
                  repeats = 5,
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 500,
                  MaxNWts = 5 * (ncol(df_Train) + 1) + 5 + 1)
#summary(NNModel_1)
# Make MARS predictions
predictions_NNModel_1 <- NNModel_1 %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
#names(predictions_NNModel_1_Acc)[names(predictions_NNModel_1_Acc) == "trainingData.y"] <- "Rsquare"
#predictions_NNModel_1_Acc
```

```{r}
```


```{r}
nnetGrid <- expand.grid(decay = c(0, 0.01, .1),
                        size = c(1:10))
# get the maximum number of hidden units
maxSize <- max(nnetGrid$size)
# compute the maximum number of parameters
# there are M(p+1)+M+1 parameters in total
numWts <- 1*(maxSize * (length(df_Train) + 1) + maxSize + 1)
ctrl <- trainControl(method = "cv",  # corss-validation
                     number = 10  # 10 folds
                     #classProbs = TRUE, # report class probability
                     #summaryFunction = twoClassSummary # return AUC
)
nnetTune <- train(PH~., data = df_Train,
                   method = "nnet", # train neural network using `nnet` package 
                   tuneGrid = nnetGrid, # tuning grid
                   trControl = ctrl, # process customization set before
                   preProc = c("center", "scale"), # standardize data
                   trace = FALSE,  # hide the training trace
                   MaxNWts = numWts,  # maximum number of weight
                   maxit = 500 # maximum iteration
)
predictions_NNModel_2 <- nnetTune %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
predictions_NNModel_1_Acc
```


====================    Tree Based Models   ===============
```{r}
ncol(df_Train)
sqrt(ncol(df_Train))
```


```{r, eval=FALSE}
tic()
rf_model <- randomForest(PH ~ ., data = df_Train, 
                       importance = TRUE,
                       ntree = 1000)
# Random Forest predictions
rf_predictions <- rf_model %>% predict(df_Test)
# Model performance metrics
rf_accuracy <- data.frame( 
  Model = "Random Forest",
  RMSE = caret::RMSE(rf_predictions, df_Test$PH),
  Rsquare = caret::R2(rf_predictions, df_Test$PH)
)
toc()
#==================
tic()
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
metric <- "RMSE"
mtry <- sqrt(ncol(df_Train))
rf_random <- train(PH ~ ., data = df_Train, 
                   method="rf", metric=metric, tuneLength=15, trControl=control)
#Random Forest predictions
rf_predictionsv2 <- rf_random %>% predict(df_Test)
# Model performance metrics
rf_accuracyV2 <- data.frame( 
  Model = "Random Forest Search",
  RMSE = caret::RMSE(rf_predictionsv2, df_Test$PH),
  Rsquare = caret::R2(rf_predictionsv2, df_Test$PH)
)
toc()
#==================
tic()
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(1:15))
rf_grid <- train(PH ~ ., data = df_Train, 
                       method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#Random Forest predictions
rf_predictionsv3 <- rf_grid %>% predict(df_Test)
# Model performance metrics
rf_accuracyV3 <- data.frame( 
  Model = "Random Forest Grid",
  RMSE = caret::RMSE(rf_predictionsv3, df_Test$PH),
  Rsquare = caret::R2(rf_predictionsv3, df_Test$PH)
)
toc()
#########
rbind(rf_accuracy,rf_accuracyV2,rf_accuracyV3)
```

Random Forest	0.1047294	0.6573319	

```{r}
tic()
##==============Single Tree==============
single_tree <- rpart(PH ~ ., data = df_Train, 
   method="anova")
# Single Tree predictions
single_t_predictions <- single_tree %>% predict(df_Test)
# Model performance metrics
single_t_accuracy <- data.frame( 
  Model = "Single Tree",
  RMSE = caret::RMSE(single_t_predictions, df_Test$PH),
  Rsquare = caret::R2(single_t_predictions, df_Test$PH)
)
##==============Random Forest==============
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(1:15))
rf_grid <- train(PH ~ ., data = df_Train, 
                       method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#Random Forest predictions
rf_predictions <- rf_grid %>% predict(df_Test)
# Model performance metrics
rf_accuracy <- data.frame( 
  Model = "Random Forest Grid",
  RMSE = caret::RMSE(rf_predictions, df_Test$PH),
  Rsquare = caret::R2(rf_predictions, df_Test$PH)
)
##==============Bagged Tree==============
bagCtrl <- cforest_control(mtry = ncol(df_Train) - 1)
baggedTree_model <- cforest(PH ~ ., data = df_Train, 
                      controls = bagCtrl)
# Bagged Tree predictions
bgTree_predictions <- baggedTree_model %>% predict(newdata = df_Test)
# Model performance metrics
bgTree_accuracy <- data.frame( 
  Model = "Bagged Tree",
  RMSE = caret::RMSE(bgTree_predictions, df_Test$PH),
  Rsquare = caret::R2(bgTree_predictions, df_Test$PH)
)
names(bgTree_accuracy)[names(bgTree_accuracy) == "PH"] <- "Rsquare"
##==============GBM==============
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
                       #.n.trees = seq(100, 1000, by = 50),
                       .n.trees = seq(100, 1000, by = 50),
                       .shrinkage = c(0.01, 0.1),
                       .n.minobsinnode = c(5, 10, 20, 30) )
gbm_model <- train(PH ~ ., data = df_Train,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 verbose = FALSE)
# GBM predictions
gbm_predictions <- gbm_model %>% predict(df_Test)
# Model performance metrics
gbm_accuracy <- data.frame( 
  Model = "Gradient Boosted Machine",
  RMSE = caret::RMSE(gbm_predictions, df_Test$PH),
  Rsquare = caret::R2(gbm_predictions, df_Test$PH)
)
##==============
tree_model_accuracy <- rbind(single_t_accuracy,rf_accuracy,bgTree_accuracy,gbm_accuracy)
tree_model_accuracy <- tree_model_accuracy[order(tree_model_accuracy$Rsquare),]
tree_model_accuracy
```



1	Single Tree	0.11984709	0.4966390	
4	Gradient Boosted Machine	0.09406770	0.6898716	
3	Bagged Tree	0.09304416	0.7033791	
2	Random Forest	0.08942298	0.7446754

29.577 sec elapsed
27.056 sec elapsed
`

Gradient Boosted Machine	0.1014709	0.6429373	#100Trees
Gradient Boosted Machine	0.0940677	0.6898716 #Trees


```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(single_t_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(rf_predictions, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(bgTree_predictions, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
plot(gbm_predictions, df_Test$PH, ylab="Observed", col = "purple")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted  - Tree Based Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```







```{r}
All_predictions <- cbind(lm_predictions,pls_predictions,ridge_predictions,rlmPCA_model_predictions,knnModel_predictions,predictions_svm,predictions_mars_tuned,predictions_NNModel_1,single_t_predictions,rf_predictions,bgTree_predictions,gbm_predictions) %>% as.data.frame()
names(All_predictions)[names(All_predictions) == 'y'] <- "predictions_mars_tuned"
names(All_predictions)[names(All_predictions) == 'PH'] <- "bgTree_predictions"
All_predictions <- cbind(All_predictions,df_Test$PH)
names(All_predictions)[names(All_predictions) == 'df_Test$PH'] <- "PH"
head(All_predictions, 5)
```


```{r}
All_predictions <- cbind(lm_predictions,pls_predictions,ridge_predictions,rlmPCA_model_predictions,knnModel_predictions,predictions_svm,predictions_mars_tuned,predictions_NNModel_1,single_t_predictions,rf_predictions,bgTree_predictions,gbm_predictions) %>% as.data.frame()
names(All_predictions)[names(All_predictions) == 'y'] <- "predictions_mars_tuned"
names(All_predictions)[names(All_predictions) == 'PH'] <- "bgTree_predictions"
All_predictions <- cbind(All_predictions,df_Test$PH)
names(All_predictions)[names(All_predictions) == 'df_Test$PH'] <- "PH"
#Melting all column values together using PH as index
df.m <- melt(All_predictions, "PH")
names(df.m)[names(df.m) == 'variable'] <- "Model"
#head(df.m)
ggplot(df.m, aes(value, PH,colour = Model)) + 
  geom_point() + 
  facet_wrap(~Model, scales = "free", ncol = 2) +
  geom_abline() +
  coord_cartesian(xlim = c(8.0, 9)) + stat_density_2d(aes(fill = ..level..), geom="polygon")
```




```{r}
all_model_accuracy <- rbind(lm_accuracy, rlmPCA_Accuracy, pls_accuracy, ridge_accuracy, predictions_NNModel_1_Acc, knn_Accuracy,SMV_Acc,MARS_Acc_tuned, single_t_accuracy,rf_accuracy,bgTree_accuracy,gbm_accuracy)
all_model_accuracy <- all_model_accuracy[order(-all_model_accuracy$Rsquare),]
all_model_accuracy
```

**With only given Predictors**
10	Random Forest	0.1044049	0.6597720	
11	Bagged Tree	0.1071411	0.6239057	
12	Gradient Boosted Machine	0.1119545	0.5820163	
7	Support Vector Machine	0.1177673	0.5381996	
6	k-Nearest Neighbors	0.1258894	0.4788588	
8	MARS Tuned	0.1290927	0.4454786	
9	Single Tree	0.1310870	0.4338619	
5	Neural Network avNNet	0.1383973	0.3706766	
3	Partial Least Squares	0.1397042	0.3519163	
1	Linear Regression	0.1398716	0.3504730	
4	Ridge-regression	0.1399046	0.3496677	
2	Robust Linear Model	0.1436261	0.3131386

**With Extra set of binary Predictors**
10	Random Forest	0.09371340	0.7293525	
11	Bagged Tree	0.09707804	0.6907065	
12	Gradient Boosted Machine	0.10191416	0.6525747	
7	Support Vector Machine	0.10960570	0.5974745	
8	MARS Tuned	0.11102573	0.5872220	
6	k-Nearest Neighbors	0.11685722	0.5468324	
9	Single Tree	0.11769607	0.5358269	
5	Neural Network avNNet	0.12279118	0.4975638	
1	Linear Regression	0.12948997	0.4370334	
4	Ridge-regression	0.12948997	0.4370334	
3	Partial Least Squares	0.12960675	0.4370189	
2	Robust Linear Model	0.13258389	0.4110927
