---
title: "DATA624_Project2"
output:
  prettydoc::html_pretty: null
  theme: architec
  toc: yes
  fig_caption: yes
  toc_collapsed: no
  toc_depth: 5
  highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache=TRUE)
#install.packages("ggcorrplot")
#install.packages(c("VIM", "rpart", "randomForest", "party", "gbm", "Cubist", "rpart.plot"))
#install.packages("PerformanceAnalytics")
library(VIM)
library(Hmisc)
#Corr Plot
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(PerformanceAnalytics)
library(magrittr)
library(reshape2)
library(ggplot2)

#Data Imputation
library(mice)

#Train model
library(caret)

#Timing
library(tictoc)

#Tree Based Plots
library(rpart)
library(randomForest)
library(party)

library(gbm)
#library(pryr)
library(Cubist)
library(Hmisc)
library(mice)
library(magrittr)
#library(partykit)
library(rpart.plot)


#Parallel Processing
library(doParallel)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
#stopCluster(cl)
set.seed(1978)
```

```{r}
hist.data.frame <- function(x, ..., colors=rainbow(ncol(x))) {
    col<-1
    hist<-function(...) {
        graphics::hist(..., col=colors[col])
        col <<- col+1
    }
    f <- Hmisc:::hist.data.frame
    environment(f) <- environment()
    f(x,...)
}
```

## Project 2: PH Prediction

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

```{r data}
training_loc <- "https://raw.githubusercontent.com/JMawyin/DATA_624/master/Student_Data_train.csv"
training_df <- read.csv(file = training_loc)
training_df$Brand.Code <- as.factor(training_df$Brand.Code)
test_loc <- "https://raw.githubusercontent.com/JMawyin/DATA_624/master/Student_Data_test.csv"
test_df <- read.csv(file = test_loc)
head(training_df)
```

```{r}
cat("Dimensions of Training DF:\n",dim(training_df))
cat("\n\nName of columns in Dataframe:\n")
colnames(training_df)
```


```{r}
table(training_df$Brand.Code)
hist(training_df$PH)
```
```{r}
dim(training_df)
```



```{r}
colSums(is.na(training_df))
```

```{r}
plot <- aggr(training_df, col = c("green", "red"), 
             numbers = TRUE, 
             sortVars = TRUE)
```


```{r}
#Moving the PH column to the front of the dataframe
training_df <- training_df[,c(which(colnames(training_df)=="PH"),which(colnames(training_df)!="PH"))]
colSums(is.na(training_df))
```


```{r}
hist(training_df[,1:9])
```

```{r}
hist(training_df[,10:18])
```

```{r}
hist(training_df[,19:27])
```

```{r}
hist(training_df[,28:33])
```

```{r}
boxplot(training_df$Filler.Speed)
```


```{r}
training_df_binary <- training_df
training_df_binary$Air.Pressurer_bin <- ifelse(training_df_binary$Air.Pressurer > 145, 1, 0)
training_df_binary$Balling.Lvl_bin <- ifelse(training_df_binary$Balling.Lvl > 2, 1, 0)
training_df_binary$Balling_bin <- ifelse(training_df_binary$Balling > 2.5, 1, 0)
training_df_binary$Density_bin <- ifelse(training_df_binary$Density > 1.25, 1, 0)
training_df_binary$Hyd.Pressure1_bin <- ifelse(training_df_binary$Hyd.Pressure1 == 0, 0, 1)
training_df_binary$Hyd.Pressure2_bin <- ifelse(training_df_binary$Hyd.Pressure2 == 0, 0, 1)
training_df_binary$Hyd.Pressure3_bin <- ifelse(training_df_binary$Hyd.Pressure3 == 0, 0, 1)
training_df_binary$Mnf.Flow_bin <- ifelse(training_df_binary$Mnf.Flow > 50, 0, 1)
dim(training_df_binary)
#hist(training_df_binary)
#hist(training_df_binary$Hyd.Pressure3)
```

```{r}
corr <- model.matrix(~0+., data=training_df_binary) %>% 
  cor(use="pairwise.complete.obs")


ggplot(melt(corr), aes(Var1, Var2, fill=value)) +
  geom_tile(height=0.8, width=0.8) +
  scale_fill_gradient2(low="blue", mid="white", high="red") +
  theme_minimal() +
  coord_equal() +
  labs(x="",y="",fill="Corr") +
  theme(axis.text.x=element_text(size=8, angle=90, vjust=1, hjust=1, 
                                 margin=ggplot2::margin(-3,0,0,0)),
        axis.text.y=element_text(size=8, margin=ggplot2::margin(0,-3,0,0)),
        panel.grid.major=element_blank()) 
```



```{r}
#Imputing using the pmm mehtod, creating 5 dataframes, do not print process
imp_training <- mice(training_df_binary, m=5, method = 'pmm',print =  FALSE)
 
# checking the summary
summary(imp_training)

# Get complete data ( 3rd out of 5)
imp_training_df <- complete(imp_training,3)
colSums(is.na(imp_training_df))
head(imp_training_df)

#A BRIEF INTRODUCTION TO MICE R PACKAGE
#https://datasciencebeginners.com/2018/11/11/a-brief-introduction-to-mice-r-package/
```






```{r, eval=FALSE}
# #Removing Highly Correlated Predictors
descrCor <-  model.matrix(~0+., data=imp_training_df) %>%
  cor(use="pairwise.complete.obs")
summary(descrCor[upper.tri(descrCor)])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .75)
imp_No_High_Corr_training_df <- imp_training_df[,-highlyCorDescr]
descrCor2 <- model.matrix(~0+., data=imp_No_High_Corr_training_df) %>%
  cor(use="pairwise.complete.obs")
summary(descrCor[upper.tri(descrCor)])
dim(imp_training_df)
dim(imp_No_High_Corr_training_df)
head(imp_No_High_Corr_training_df)

#Pre Prcessing Predictors
pp_no_nzv <- preProcess(imp_No_High_Corr_training_df[, 2:27],
                        method = c("center", "scale")) #no  "nzv" "BoxCox"  "YeoJohnson"

pp_no_nzv
Pre_processed_imp_No_High_Corr_training_df <- predict(pp_no_nzv, newdata = imp_No_High_Corr_training_df[, 1:27])
```






```{r}
#training_df
#imp_training_df
#imp_No_High_Corr_training_df
#Pre_processed_imp_No_High_Corr_training_df
df <- imp_training_df
#df <- ppc_df

#Creating Test and Training Set
trainIndex <- createDataPartition(df$PH, p = .8, 
                                  list = FALSE, 
                                  times = 1)
df_Train <- df[ trainIndex,]
df_Test  <- df[-trainIndex,]
```



=================
```{r}
# model1 <- lm(PH ~., data = df_Train)
# predictions <- model1 %>% predict(df_Test)
# data.frame(
#   RMSE = RMSE(predictions, df_Test$PH),
#   R2 = R2(predictions, df_Test$PH)
# )
# car::vif(model1)
```
```{r}
# model2 <- lm(PH ~. -Carb.Pressure, data = df_Train)
# predictions2 <- model2 %>% predict(df_Test)
# data.frame(
#   RMSE = RMSE(predictions2, df_Test$PH),
#   R2 = R2(predictions2, df_Test$PH)
# )
# car::vif(model2)
```


=================

```{r}
tic()
######## Train Models

ctrl <- trainControl(method = "cv", number = 10)
#Linear Model
lm_model <- train(
  PH~., data = df_Train, method = "lm")
#Partial Least Squares
pls_model <- train(PH~., data = df_Train,
                  method = "pls",
                  ## The default tuning grid evaluates
                  ## components 1... tuneLength
                  tuneLength = 20,
                  trControl = ctrl,
                  preProc = c("center", "scale"))
#Ridge Regression
# Define the candidate set of values
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
set.seed(100)
ridge_model <- train(PH~., data = df_Train,
                      method = "ridge",
                      ## Fir the model over many penalty values
                      tuneGrid = ridgeGrid,
                      trControl = ctrl,
                      ## put the predictors on the same scale
                      preProc = c("center", "scale"))
#Robust Linear Regression
ctrl <- trainControl(method = "cv", number = 10)
rlmPCA_model <- train(PH~., data = df_Train,
                method = "rlm",
                preProcess = "pca",
                trControl = ctrl)

########  Make predictions

lm_predictions <- lm_model %>% predict(df_Test)
# Model performance metrics
lm_accuracy <- data.frame( 
  Model = "Linear Regression",
  RMSE = caret::RMSE(lm_predictions, df_Test$PH),
  Rsquare = caret::R2(lm_predictions, df_Test$PH)
)
#lm_accuracy
# Make predictions
pls_predictions <- pls_model %>% predict(df_Test)
# Model performance metrics
pls_accuracy <- data.frame( 
  Model = "Partial Least Squares",
  RMSE = caret::RMSE(pls_predictions, df_Test$PH),
  Rsquare = caret::R2(pls_predictions, df_Test$PH)
)

ridge_predictions <- ridge_model %>% predict(df_Test)
# Model performance metrics
ridge_accuracy <- data.frame( 
  Model = "Ridge-regression",
  RMSE = caret::RMSE(ridge_predictions, df_Test$PH),
  Rsquare = caret::R2(ridge_predictions, df_Test$PH)
)
rlmPCA_model_predictions <- rlmPCA_model %>% predict(df_Test)

cat("All four linear regression models took ") 
toc()
cat("to train.\n")

# Model performance metrics
rlmPCA_Accuracy <- data.frame(
  Model = "Robust Linear Model",
  RMSE = caret::RMSE(rlmPCA_model_predictions,df_Test$PH),
  Rsquare = caret::R2(rlmPCA_model_predictions,df_Test$PH))
rbind(lm_accuracy, rlmPCA_Accuracy, pls_accuracy, ridge_accuracy)
```
```{r}
dim(training_df)
dim(imp_training_df)
```


#Inputed Predictors and Binary Predictors
Linear Regression	0.1328836	0.4088347		
Robust Linear Model	0.1369765	0.3703304		
Partial Least Squares	0.1332510	0.4052933		
Ridge-regression	0.1330168	0.4070971	

Linear Regression	0.1294900	0.4370334		
Robust Linear Model	0.1325839	0.4110927		
Partial Least Squares	0.1296068	0.4370189		
Ridge-regression	0.1294900	0.4370334

Linear Regression	0.1294900	0.4370334		
Robust Linear Model	0.1325839	0.4110927		
Partial Least Squares	0.1296068	0.4370189		
Ridge-regression	0.1294900	0.4370334	

#Imputed Predictors
Linear Regression	0.1247379	0.4541905		
Robust Linear Model	0.1304918	0.4027846		
Partial Least Squares	0.1251459	0.4506813		
Ridge-regression	0.1249724	0.4521616		

Linear Regression	0.1191384	0.4442342		
Robust Linear Model	0.1267581	0.3699280		
Partial Least Squares	0.1195921	0.4402759		
Ridge-regression	0.1192468	0.4433601	

Linear Regression	0.1245448	0.4612370		
Robust Linear Model	0.1319684	0.3944299		
Partial Least Squares	0.1250159	0.4567820		
Ridge-regression	0.1248693	0.4585202	

Linear Regression	0.1398716	0.3504730		
Robust Linear Model	0.1436261	0.3131386		
Partial Least Squares	0.1397042	0.3519163		
Ridge-regression	0.1399046	0.3496677	

#imp_training_df
Linear Regression	0.1398716	0.3504730		
Robust Linear Model	0.1436261	0.3131386		
Partial Least Squares	0.1397042	0.3519163		
Ridge-regression	0.1399046	0.3496677

*imp_No_High_Corr_training_df
Linear Regression	0.1471077	0.2807598		
Robust Linear Model	0.1481916	0.2684600		
Partial Least Squares	0.1472959	0.2788179		
Ridge-regression	0.1473724	0.2779339

*Pre_processed_imp_No_High_Corr_training_df
Linear Regression	0.1481782	0.2720243		
Robust Linear Model	0.1474636	0.2758037		
Partial Least Squares	0.1478913	0.2743631		
Ridge-regression	0.1479631	0.2733337



Linear Regression	0.1481782	0.2720243		
Robust Linear Model	0.1474636	0.2758037		
Partial Least Squares	0.1478913	0.2743631		
Ridge-regression	0.1479631	0.2733337	

Linear Regression	0.1314113	0.3591871		
Robust Linear Model	0.1360182	0.3135105		
Partial Least Squares	0.1313908	0.3597019		
Ridge-regression	0.1312644	0.3599552	

Linear Regression	0.1357174	0.3887110		
Robust Linear Model	0.1398007	0.3522298		
Partial Least Squares	0.1354026	0.3915294		
Ridge-regression	0.1356460	0.3893101

```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))

plot(lm_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(pls_predictions, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
plot(ridge_predictions, df_Test$PH, ylab="Observed", col = "purple")
abline(0, 1, lwd=2)
plot(rlmPCA_model_predictions, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted", side = 3, line = -2, outer = TRUE)
```

```{r}
plot(varImp(lm_model), main="Linear Regression Predictor Importance", top = 5, xlim = c(0,100))

plot(varImp(ridge_model), main="Ridge Regression Predictor Importance", top = 5, xlim = c(0,100))
```



```{r}
tic()
knnModel <- train(PH~., data = df_Train,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)
#knnModel
tc <- trainControl(method = "cv",
                           number = 5,
                           classProbs = T)
svmModel <- train(PH~., data = df_Train,
                    method = "svmRadial",
                    preProcess = c("BoxCox","center", "scale"),
                    trControl = tc,
                    tuneLength = 9)

marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(PH~., data = df_Train,
                   method = "earth",
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = "cv"))

cat("All 3 non-linear regression models took ") 
toc()
cat("to train.\n")
```

```{r}

knnModel_predictions <- knnModel %>% predict(df_Test)
# Model performance metrics
knn_Accuracy <- data.frame(
  Model = "k-Nearest Neighbors",
  RMSE = caret::RMSE(knnModel_predictions,df_Test$PH),
  Rsquare = caret::R2(knnModel_predictions,df_Test$PH))

predictions_svm <- svmModel %>% predict(df_Test)
# Model SVM performance metrics
SMV_Acc <- data.frame(
  Model = "Support Vector Machine",
  RMSE = caret::RMSE(predictions_svm, df_Test$PH),
  Rsquare = caret::R2(predictions_svm, df_Test$PH)
)

#summary(marsTuned)
# Make MARS predictions
predictions_mars_tuned <- marsTuned %>% predict(df_Test)
# Model MARS performance metrics
MARS_Acc_tuned <- data.frame(
  Model = "MARS Tuned",
  RMSE = caret::RMSE(predictions_mars_tuned, df_Test$PH),
  Rsquare = caret::R2(predictions_mars_tuned, df_Test$PH)
)
names(MARS_Acc_tuned)[names(MARS_Acc_tuned) == 'y'] <- "Rsquare"


rbind(knn_Accuracy,SMV_Acc,MARS_Acc_tuned)
```

#Inputed Predictors and Binary Predictors

#Inputed Predictors
k-Nearest Neighbors	0.1082467	0.5914802		
Support Vector Machine	0.1066275	0.6045377		
MARS Tuned	0.1147101	0.5398671	

k-Nearest Neighbors	0.1266089	0.4806607		
Support Vector Machine	0.1203518	0.5272374		
MARS Tuned	0.1300447	0.4443375	

```{r}
ncol(df_Train)
5 * (ncol(df_Train) + 1) + 5 + 1
```

```{r}
NNModel_1 <- avNNet(PH~., data = df_Train,
                   size = 5, 
                   decay = 0.01,
                   linout = TRUE, 
                   trace = FALSE,
                   maxit = 500)
predictions_NNModel_1 <- NNModel_1 %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
predictions_NNModel_1_Acc
```


```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))

plot(knnModel_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(predictions_svm, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(predictions_mars_tuned, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)

mtext("Observed Vs. Predicted  - Non - Linar Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```


```{r, eval = FALSE}

NNModel_1 <- avNNet(PH~., data = df_Train,
                  #trainingData$x, trainingData$y,
                  size = 5,
                  decay = 0.01,
                  repeats = 5,
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 500,
                  MaxNWts = 5 * (ncol(df_Train) + 1) + 5 + 1)
#summary(NNModel_1)
# Make MARS predictions
predictions_NNModel_1 <- NNModel_1 %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
#names(predictions_NNModel_1_Acc)[names(predictions_NNModel_1_Acc) == "trainingData.y"] <- "Rsquare"
#predictions_NNModel_1_Acc
```

```{r}

```


```{r}
nnetGrid <- expand.grid(decay = c(0, 0.01, .1),
                        size = c(1:10))
# get the maximum number of hidden units
maxSize <- max(nnetGrid$size)
# compute the maximum number of parameters
# there are M(p+1)+M+1 parameters in total
numWts <- 1*(maxSize * (length(df_Train) + 1) + maxSize + 1)


ctrl <- trainControl(method = "cv",  # corss-validation
                     number = 10  # 10 folds
                     #classProbs = TRUE, # report class probability
                     #summaryFunction = twoClassSummary # return AUC
)

nnetTune <- train(PH~., data = df_Train,
                   method = "nnet", # train neural network using `nnet` package 
                   tuneGrid = nnetGrid, # tuning grid
                   trControl = ctrl, # process customization set before
                   preProc = c("center", "scale"), # standardize data
                   trace = FALSE,  # hide the training trace
                   MaxNWts = numWts,  # maximum number of weight
                   maxit = 500 # maximum iteration
)
predictions_NNModel_2 <- nnetTune %>% predict(df_Test)
# Model MARS performance metrics
predictions_NNModel_1_Acc <- data.frame(
  Model = "Neural Network avNNet",
  RMSE = caret::RMSE(predictions_NNModel_1, df_Test$PH),
  Rsquare = caret::R2(predictions_NNModel_1, df_Test$PH)
)
predictions_NNModel_1_Acc
```


====================    Tree Based Models   ===============
```{r}
ncol(df_Train)
sqrt(ncol(df_Train))
```


```{r, eval=FALSE}
tic()
rf_model <- randomForest(PH ~ ., data = df_Train, 
                       importance = TRUE,
                       ntree = 1000)
# Random Forest predictions
rf_predictions <- rf_model %>% predict(df_Test)
# Model performance metrics
rf_accuracy <- data.frame( 
  Model = "Random Forest",
  RMSE = caret::RMSE(rf_predictions, df_Test$PH),
  Rsquare = caret::R2(rf_predictions, df_Test$PH)
)

toc()
#==================
tic()
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
metric <- "RMSE"
mtry <- sqrt(ncol(df_Train))
rf_random <- train(PH ~ ., data = df_Train, 
                   method="rf", metric=metric, tuneLength=15, trControl=control)
#Random Forest predictions
rf_predictionsv2 <- rf_random %>% predict(df_Test)
# Model performance metrics
rf_accuracyV2 <- data.frame( 
  Model = "Random Forest Search",
  RMSE = caret::RMSE(rf_predictionsv2, df_Test$PH),
  Rsquare = caret::R2(rf_predictionsv2, df_Test$PH)
)

toc()
#==================
tic()
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

tunegrid <- expand.grid(.mtry=c(1:15))
rf_grid <- train(PH ~ ., data = df_Train, 
                       method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#Random Forest predictions
rf_predictionsv3 <- rf_grid %>% predict(df_Test)
# Model performance metrics
rf_accuracyV3 <- data.frame( 
  Model = "Random Forest Grid",
  RMSE = caret::RMSE(rf_predictionsv3, df_Test$PH),
  Rsquare = caret::R2(rf_predictionsv3, df_Test$PH)
)
toc()
#########
rbind(rf_accuracy,rf_accuracyV2,rf_accuracyV3)
```

Random Forest	0.1047294	0.6573319	

```{r}
tic()
##==============Single Tree==============
single_tree <- rpart(PH ~ ., data = df_Train, 
   method="anova")
# Single Tree predictions
single_t_predictions <- single_tree %>% predict(df_Test)
# Model performance metrics
single_t_accuracy <- data.frame( 
  Model = "Single Tree",
  RMSE = caret::RMSE(single_t_predictions, df_Test$PH),
  Rsquare = caret::R2(single_t_predictions, df_Test$PH)
)
##==============Random Forest==============
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

tunegrid <- expand.grid(.mtry=c(1:15))
rf_grid <- train(PH ~ ., data = df_Train, 
                       method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
#Random Forest predictions
rf_predictions <- rf_grid %>% predict(df_Test)
# Model performance metrics
rf_accuracy <- data.frame( 
  Model = "Random Forest Grid",
  RMSE = caret::RMSE(rf_predictions, df_Test$PH),
  Rsquare = caret::R2(rf_predictions, df_Test$PH)
)
##==============Bagged Tree==============
bagCtrl <- cforest_control(mtry = ncol(df_Train) - 1)
baggedTree_model <- cforest(PH ~ ., data = df_Train, 
                      controls = bagCtrl)
# Bagged Tree predictions
bgTree_predictions <- baggedTree_model %>% predict(newdata = df_Test)
# Model performance metrics
bgTree_accuracy <- data.frame( 
  Model = "Bagged Tree",
  RMSE = caret::RMSE(bgTree_predictions, df_Test$PH),
  Rsquare = caret::R2(bgTree_predictions, df_Test$PH)
)
names(bgTree_accuracy)[names(bgTree_accuracy) == "PH"] <- "Rsquare"
##==============GBM==============
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
                       #.n.trees = seq(100, 1000, by = 50),
                       .n.trees = seq(100, 1000, by = 50),
                       .shrinkage = c(0.01, 0.1),
                       .n.minobsinnode = c(5, 10, 20, 30) )
gbm_model <- train(PH ~ ., data = df_Train,
                 method = "gbm",
                 tuneGrid = gbmGrid,
                 verbose = FALSE)
# GBM predictions
gbm_predictions <- gbm_model %>% predict(df_Test)
# Model performance metrics
gbm_accuracy <- data.frame( 
  Model = "Gradient Boosted Machine",
  RMSE = caret::RMSE(gbm_predictions, df_Test$PH),
  Rsquare = caret::R2(gbm_predictions, df_Test$PH)
)
##==============
tree_model_accuracy <- rbind(single_t_accuracy,rf_accuracy,bgTree_accuracy,gbm_accuracy)
tree_model_accuracy <- tree_model_accuracy[order(tree_model_accuracy$Rsquare),]
tree_model_accuracy
```



1	Single Tree	0.11984709	0.4966390	
4	Gradient Boosted Machine	0.09406770	0.6898716	
3	Bagged Tree	0.09304416	0.7033791	
2	Random Forest	0.08942298	0.7446754

29.577 sec elapsed
27.056 sec elapsed
`

Gradient Boosted Machine	0.1014709	0.6429373	#100Trees
Gradient Boosted Machine	0.0940677	0.6898716 #Trees


```{r}
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))

plot(single_t_predictions, df_Test$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(rf_predictions, df_Test$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(bgTree_predictions, df_Test$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
plot(gbm_predictions, df_Test$PH, ylab="Observed", col = "purple")
abline(0, 1, lwd=2)

mtext("Observed Vs. Predicted  - Tree Based Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
```







```{r}
All_predictions <- cbind(lm_predictions,pls_predictions,ridge_predictions,rlmPCA_model_predictions,knnModel_predictions,predictions_svm,predictions_mars_tuned,predictions_NNModel_1,single_t_predictions,rf_predictions,bgTree_predictions,gbm_predictions) %>% as.data.frame()
names(All_predictions)[names(All_predictions) == 'y'] <- "predictions_mars_tuned"
names(All_predictions)[names(All_predictions) == 'PH'] <- "bgTree_predictions"
All_predictions <- cbind(All_predictions,df_Test$PH)
names(All_predictions)[names(All_predictions) == 'df_Test$PH'] <- "PH"
head(All_predictions, 5)

```


```{r}
All_predictions <- cbind(lm_predictions,pls_predictions,ridge_predictions,rlmPCA_model_predictions,knnModel_predictions,predictions_svm,predictions_mars_tuned,predictions_NNModel_1,single_t_predictions,rf_predictions,bgTree_predictions,gbm_predictions) %>% as.data.frame()
names(All_predictions)[names(All_predictions) == 'y'] <- "predictions_mars_tuned"
names(All_predictions)[names(All_predictions) == 'PH'] <- "bgTree_predictions"
All_predictions <- cbind(All_predictions,df_Test$PH)
names(All_predictions)[names(All_predictions) == 'df_Test$PH'] <- "PH"

#Melting all column values together using PH as index
df.m <- melt(All_predictions, "PH")
names(df.m)[names(df.m) == 'variable'] <- "Model"
#head(df.m)

ggplot(df.m, aes(value, PH,colour = Model)) + 
  geom_point() + 
  facet_wrap(~Model, scales = "free", ncol = 2) +
  geom_abline() +
  coord_cartesian(xlim = c(8.0, 9)) + stat_density_2d(aes(fill = ..level..), geom="polygon")
```




```{r}
all_model_accuracy <- rbind(lm_accuracy, rlmPCA_Accuracy, pls_accuracy, ridge_accuracy, predictions_NNModel_1_Acc, knn_Accuracy,SMV_Acc,MARS_Acc_tuned, single_t_accuracy,rf_accuracy,bgTree_accuracy,gbm_accuracy)
all_model_accuracy <- all_model_accuracy[order(-all_model_accuracy$Rsquare),]
all_model_accuracy
```

**With only given Predictors**
10	Random Forest	0.1044049	0.6597720	
11	Bagged Tree	0.1071411	0.6239057	
12	Gradient Boosted Machine	0.1119545	0.5820163	
7	Support Vector Machine	0.1177673	0.5381996	
6	k-Nearest Neighbors	0.1258894	0.4788588	
8	MARS Tuned	0.1290927	0.4454786	
9	Single Tree	0.1310870	0.4338619	
5	Neural Network avNNet	0.1383973	0.3706766	
3	Partial Least Squares	0.1397042	0.3519163	
1	Linear Regression	0.1398716	0.3504730	
4	Ridge-regression	0.1399046	0.3496677	
2	Robust Linear Model	0.1436261	0.3131386

**With Extra set of binary Predictors**
10	Random Forest	0.09371340	0.7293525	
11	Bagged Tree	0.09707804	0.6907065	
12	Gradient Boosted Machine	0.10191416	0.6525747	
7	Support Vector Machine	0.10960570	0.5974745	
8	MARS Tuned	0.11102573	0.5872220	
6	k-Nearest Neighbors	0.11685722	0.5468324	
9	Single Tree	0.11769607	0.5358269	
5	Neural Network avNNet	0.12279118	0.4975638	
1	Linear Regression	0.12948997	0.4370334	
4	Ridge-regression	0.12948997	0.4370334	
3	Partial Least Squares	0.12960675	0.4370189	
2	Robust Linear Model	0.13258389	0.4110927

